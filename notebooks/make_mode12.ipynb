{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe406cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from XGBoost GridSearchCV: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 300, 'reg_alpha': 0.01, 'reg_lambda': 0.1, 'subsample': 1.0}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'eval_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     47\u001b[39m params = {\n\u001b[32m     48\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meval_metric\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Use RMSE for evaluation\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mearly_stopping_rounds\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m50\u001b[39m  \u001b[38;5;66;03m# Stop after 50 rounds with no improvement\u001b[39;00m\n\u001b[32m     50\u001b[39m }\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Fit the model with early stopping using eval_set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mbest_xgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include eval_metric and early_stopping_rounds\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Make predictions and evaluate the model\u001b[39;00m\n\u001b[32m     61\u001b[39m y_pred = best_xgb_model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: XGBModel.fit() got an unexpected keyword argument 'eval_metric'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load processed data\n",
    "df = pd.read_csv('../data/processed/ml_data12.csv')\n",
    "\n",
    "# Prepare training data (features and target)\n",
    "X = df.drop(columns=['universityRankingNum', 'uniqueID'])\n",
    "y = df['universityRankingNum']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for XGBoost using GridSearchCV\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 300, 500, 700],  # Higher number of estimators\n",
    "    'max_depth': [3, 5, 6, 10],  # Depth control for trees\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],  # Subsampling\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],  # Feature subsampling\n",
    "    'reg_alpha': [0.01, 0.1, 1.0],  # L1 regularization\n",
    "    'reg_lambda': [0.01, 0.1, 1.0]  # L2 regularization\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from GridSearchCV\n",
    "print(f\"Best parameters from XGBoost GridSearchCV: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the best XGBoost model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Training with early stopping\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]  # Evaluation set\n",
    "\n",
    "# Specify the parameters for early stopping and eval_metric\n",
    "params = {\n",
    "    'eval_metric': 'rmse',  # Use RMSE for evaluation\n",
    "    'early_stopping_rounds': 50  # Stop after 50 rounds with no improvement\n",
    "}\n",
    "\n",
    "# Fit the model with early stopping using eval_set\n",
    "best_xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=True,\n",
    "    **params  # Include eval_metric and early_stopping_rounds\n",
    ")\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✅ XGBoost Test RMSE: {rmse:.2f}\")\n",
    "print(f\"✅ XGBoost R²: {r2:.2f}\")\n",
    "\n",
    "# Save the best XGBoost model\n",
    "joblib.dump(best_xgb_model, '../models/xgboost_model_best.joblib')\n",
    "print(\"✅ Best XGBoost model saved to '../models/xgboost_model_best.joblib'\")\n",
    "\n",
    "# Optional: Evaluate feature importance after training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importance\n",
    "xgb.plot_importance(best_xgb_model, importance_type='weight', max_num_features=10)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
